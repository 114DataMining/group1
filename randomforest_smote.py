import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os  # [æ–°å¢] ç”¨ä¾†é¡¯ç¤ºæª”æ¡ˆè·¯å¾‘
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler

# å¼•å…¥ Pipeline ç¢ºä¿é †åºæ­£ç¢º
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import BorderlineSMOTE

# ==========================================
# 1. è®€å–è³‡æ–™èˆ‡åŸºç¤è¨­å®š
# ==========================================
# è«‹ç¢ºèª csv æª”æ¡ˆè·Ÿç¨‹å¼ç¢¼åœ¨åŒä¸€å€‹è³‡æ–™å¤¾
df = pd.read_csv("diabetes_skinthickness_knn_imputed.csv")
target = "Outcome"

X = df.drop(columns=[target])
y = df[target]

# 2. åˆ‡åˆ† 80% è¨“ç·´é›†, 20% æ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("-" * 60)
print("[1] åŸå§‹è³‡æ–™åˆ‡åˆ†ç‹€æ³")
print("-" * 60)
print(f"å®Œæ•´è¨“ç·´é›† (80%): {len(y_train)} ç­†")
print(f"   é¡åˆ¥ 0 (æ²’ç—…): {sum(y_train==0)} ç­†")
print(f"   é¡åˆ¥ 1 (æœ‰ç—…): {sum(y_train==1)} ç­†")
print(f"æ¸¬è©¦é›† (20%): {len(y_test)} ç­†")

# ==========================================
# 3. GridSearchCV å°‹æ‰¾æœ€ä½³è¶…åƒæ•¸
# ==========================================
print("\n" + "-" * 60)
print("[2] åŸ·è¡Œ GridSearchCV å°‹æ‰¾æœ€ä½³è¶…åƒæ•¸")
print("-" * 60)

# å»ºç«‹åŸºç¤ Pipeline
pipeline_grid = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', BorderlineSMOTE(random_state=42, kind='borderline-1')),
    ('rf', RandomForestClassifier(random_state=42, n_jobs=-1, class_weight=None)) 
])

# è¨­å®šæ“´å……å¾Œçš„è¶…åƒæ•¸ç¯„åœ
param_grid = {
    'rf__n_estimators': [100, 200, 300],        # æ¨¹çš„æ•¸é‡
    'rf__max_depth': [5, 10, None],             # æ¨¹çš„æ·±åº¦
    'rf__min_samples_split': [2, 5],            # ç¯€é»åˆ†è£‚æ‰€éœ€æœ€å°æ¨£æœ¬æ•¸
    'rf__min_samples_leaf': [1, 2, 4],          # è‘‰å­ç¯€é»æœ€å°‘æ¨£æœ¬æ•¸
    'rf__max_features': ['sqrt', 'log2']        # æœ€å¤§ç‰¹å¾µæ•¸é¸æ“‡
}

# åŸ·è¡Œæœå°‹ (ä¿®æ­£: æ”¹å› f1 ä»¥é‡å°ä¸å¹³è¡¡è³‡æ–™)
grid_search = GridSearchCV(
    estimator=pipeline_grid,
    param_grid=param_grid,
    cv=5, 
    scoring='roc_auc',  # â˜… é€™è£¡å¹«ä½ æ”¹å›ä¾†äº†ï¼ŒåŸæœ¬ 'None' æœƒå ±éŒ¯
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# åˆ—å‡ºæ‰€æœ‰è¶…åƒæ•¸é…å°çµæœ
print("\n[è©³ç´°å ±å‘Š] æ¯å€‹è¶…åƒæ•¸çµ„åˆçš„æ¸¬è©¦çµæœ (æŒ‰ F1 åˆ†æ•¸æ’åº):")
results_df = pd.DataFrame(grid_search.cv_results_)
cols_to_show = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
print(results_df[cols_to_show].sort_values(by='rank_test_score').to_string(index=False))

# å–å¾—æœ€ä½³åƒæ•¸èˆ‡æ¨¡å‹
best_params = grid_search.best_params_
best_model_pipeline = grid_search.best_estimator_

print(f"\næœ€çµ‚é¸å®šæœ€ä½³åƒæ•¸: {best_params}")
print(f"æœ€ä½³ CV F1 Score: {grid_search.best_score_:.4f}")

# ==========================================
# 4. ä½¿ç”¨ã€Œæœ€ä½³åƒæ•¸ã€åŸ·è¡Œè©³ç´°çš„ 5-Fold CV (å«è¨“ç·´é›†èˆ‡é©—è­‰é›†æ¯”è¼ƒ)
# ==========================================
print("\n" + "=" * 80)
print("[3] é‡å°ã€Œæœ€ä½³åƒæ•¸ã€åŸ·è¡Œ 5-Fold CV è©³ç´°åˆ†æ (Train vs Validation)")
print("=" * 80)

# åˆå§‹åŒ–å„²å­˜æŒ‡æ¨™çš„ List (è¨“ç·´é›†ç”¨)
tr_accs, tr_f1s, tr_aucs = [], [], []
# åˆå§‹åŒ–å„²å­˜æŒ‡æ¨™çš„ List (é©—è­‰é›†ç”¨)
val_accs, val_f1s, val_aucs = [], [], []

# åˆå§‹åŒ–ç•«åœ–ç”¨çš„è®Šæ•¸
tprs = []
mean_fpr = np.linspace(0, 1, 100)
fig1, ax1 = plt.subplots(figsize=(10, 8))

# è¨­å®š K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):
    # 1. åˆ‡åˆ†è³‡æ–™
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

    # 2. é¡¯ç¤º SMOTE å¹³è¡¡å‰å¾Œçš„ 0/1 æ•¸é‡ (åƒ…ä¾›é¡¯ç¤ºç”¨ï¼Œå¯¦éš›å¹³è¡¡ç”± Pipeline è™•ç†)
    # ç‚ºäº†é¡¯ç¤ºæ•¸é‡ï¼Œæˆ‘å€‘åœ¨é€™è£¡æ‰‹å‹•åšä¸€æ¬¡ SMOTE check
    smote_check = BorderlineSMOTE(random_state=42, kind='borderline-1')
    X_tr_res_check, y_tr_res_check = smote_check.fit_resample(X_tr, y_tr)
    
    print(f"\nğŸ”¹ [Fold {fold}] æ•¸æ“šåˆ†ä½ˆè©³æƒ…:")
    print(f"   ã€å¹³è¡¡å‰ã€‘ 0 (æ²’ç—…): {sum(y_tr==0)} | 1 (æœ‰ç—…): {sum(y_tr==1)}")
    print(f"   ã€å¹³è¡¡å¾Œã€‘ 0 (æ²’ç—…): {sum(y_tr_res_check==0)} | 1 (æœ‰ç—…): {sum(y_tr_res_check==1)}")

    # 3. è¨“ç·´æ¨¡å‹ (Pipeline æœƒè‡ªå‹•è™•ç† Scaling -> SMOTE -> RF)
    best_model_pipeline.fit(X_tr, y_tr)

    # 4. === è¨ˆç®— [è¨“ç·´é›† Training Set] æŒ‡æ¨™ (è‡ªå·±è€ƒè‡ªå·±) ===
    y_tr_pred = best_model_pipeline.predict(X_tr)
    y_tr_prob = best_model_pipeline.predict_proba(X_tr)[:, 1]
    
    tr_acc = accuracy_score(y_tr, y_tr_pred)
    tr_f1 = f1_score(y_tr, y_tr_pred)
    tr_auc = roc_auc_score(y_tr, y_tr_prob)
    
    tr_accs.append(tr_acc)
    tr_f1s.append(tr_f1)
    tr_aucs.append(tr_auc)

    # 5. === è¨ˆç®— [é©—è­‰é›† Validation Set] æŒ‡æ¨™ (æ¨¡æ“¬è€ƒ) ===
    y_val_pred = best_model_pipeline.predict(X_val)
    y_val_prob = best_model_pipeline.predict_proba(X_val)[:, 1]
    
    val_acc = accuracy_score(y_val, y_val_pred)
    val_f1 = f1_score(y_val, y_val_pred)
    val_auc = roc_auc_score(y_val, y_val_prob)
    
    val_accs.append(val_acc)
    val_f1s.append(val_f1)
    val_aucs.append(val_auc)

    # 6. å°å‡ºè©²æŠ˜çµæœæ¯”è¼ƒ
    print(f"   ğŸ“Š æŒ‡æ¨™æ¯”è¼ƒ:")
    print(f"      Train (è¨“ç·´): Acc={tr_acc:.4f} | F1={tr_f1:.4f} | AUC={tr_auc:.4f}")
    print(f"      Valid (é©—è­‰): Acc={val_acc:.4f} | F1={val_f1:.4f} | AUC={val_auc:.4f}")

    # 7. ç•« ROC ç·š (åªç•«é©—è­‰é›†çš„)
    fpr, tpr, _ = roc_curve(y_val, y_val_prob)
    ax1.plot(fpr, tpr, alpha=0.3, label=f'Fold {fold} (AUC = {val_auc:.2f})')
    interp_tpr = np.interp(mean_fpr, fpr, tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)

# --- ç¹ªåœ–æ”¶å°¾ ---
ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)
mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(val_aucs)
ax1.plot(mean_fpr, mean_tpr, color='b', label=f'Mean ROC (AUC = {mean_auc:.2f} +/- {std_auc:.2f})', lw=2, alpha=.8)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
ax1.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='+/- 1 std. dev.')

ax1.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title="ROC Curve (5-Fold CV - Validation)", xlabel='False Positive Rate', ylabel='True Positive Rate')
ax1.legend(loc="lower right")
plt.show()

# --- æœ€çµ‚çµ±è¨ˆè¡¨æ ¼ ---
print("\n" + "="*60)
print("             ã€5-Fold CV æœ€çµ‚å¹³å‡çµæœçµ±è¨ˆã€‘")
print("="*60)
print(f"{'Metric':<10} | {'Training Set (Mean Â± Std)':<25} | {'Validation Set (Mean Â± Std)':<25}")
print("-" * 65)
print(f"{'Accuracy':<10} | {np.mean(tr_accs):.4f} Â± {np.std(tr_accs):.4f}      | {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}")
print(f"{'F1 Score':<10} | {np.mean(tr_f1s):.4f} Â± {np.std(tr_f1s):.4f}      | {np.mean(val_f1s):.4f} Â± {np.std(val_f1s):.4f}")
print(f"{'AUC':<10}      | {np.mean(tr_aucs):.4f} Â± {np.std(tr_aucs):.4f}      | {np.mean(val_aucs):.4f} Â± {np.std(val_aucs):.4f}")
print("="*60)
print("ğŸ’¡ è§€å¯Ÿé‡é»ï¼šå¦‚æœ Training åˆ†æ•¸é é«˜æ–¼ Validation åˆ†æ•¸ (ä¾‹å¦‚å·® 0.1 ä»¥ä¸Š)ï¼Œ")
print("             å‰‡æ¨¡å‹å¯èƒ½å­˜åœ¨éæ“¬åˆ (Overfitting) ç¾è±¡ã€‚")

# ==========================================
# 5. æœ€çµ‚æ¨¡å‹è¨“ç·´èˆ‡æ¨™æº–æ¸¬è©¦ (Threshold = 0.5)
# ==========================================
print("\n" + "-" * 60)
print("[4] æœ€çµ‚æ¨¡å‹è¨“ç·´èˆ‡æ¨™æº–æ¸¬è©¦ (Threshold = 0.5)")
print("-" * 60)

# 1. æª¢æŸ¥æœ€çµ‚è¨“ç·´é›† SMOTE å¾Œçš„æ•¸é‡
print("æ­£åœ¨å°å®Œæ•´ 80% è¨“ç·´é›†é€²è¡Œ SMOTE æ•¸é‡æª¢æŸ¥...")
smote_final_check = BorderlineSMOTE(random_state=42, kind='borderline-1')
X_final_res, y_final_res = smote_final_check.fit_resample(X_train, y_train)
print(f"   åŸå§‹è¨“ç·´é›†åˆ†ä½ˆ: 0={sum(y_train==0)}, 1={sum(y_train==1)}")
print(f"   å¹³è¡¡å¾Œè¨“ç·´é›†åˆ†ä½ˆ: 0={sum(y_final_res==0)}, 1={sum(y_final_res==1)}")

# 2. è¨“ç·´æœ€çµ‚æ¨¡å‹ (ä½¿ç”¨æœ€ä½³åƒæ•¸)
best_model_pipeline.fit(X_train, y_train)

# 3. é æ¸¬æ¸¬è©¦é›† (ç”¢ç”Ÿæ©Ÿç‡å€¼)
y_prob_test = best_model_pipeline.predict_proba(X_test)[:, 1]

# 4. ç”¢ç”Ÿæ¨™æº–é æ¸¬çµæœ (é–¾å€¼ 0.5)
y_pred_std = best_model_pipeline.predict(X_test)

# 5. è¨ˆç®—æ¨™æº–æŒ‡æ¨™
acc_std = accuracy_score(y_test, y_pred_std)
f1_std = f1_score(y_test, y_pred_std)
auc_std = roc_auc_score(y_test, y_prob_test)

print(f"\n[æ¨™æº–æ¸¬è©¦é›†çµæœ (Threshold = 0.5)]")
print(f"Accuracy: {acc_std:.4f}")
print(f"AUC:      {auc_std:.4f}")
print(f"F1 Score: {f1_std:.4f}")

# 6. æ¨™æº–æ··æ·†çŸ©é™£
cm_std = confusion_matrix(y_test, y_pred_std)
tn, fp, fn, tp = cm_std.ravel()
print(f"æ··æ·†çŸ©é™£:\n TN={tn} | FP={fp}\n FN={fn} | TP={tp}")

# 7. ç¹ªè£½æ¨™æº– ROC æ›²ç·š
fig2, ax2 = plt.subplots(figsize=(10, 8))
fpr, tpr, thresholds = roc_curve(y_test, y_prob_test)
ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'Test ROC (AUC = {auc_std:.2f})')
ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
# æ¨™è¨˜ 0.5 çš„ä½ç½®
idx_05 = np.argmin(np.abs(thresholds - 0.5))
ax2.scatter(fpr[idx_05], tpr[idx_05], s=100, c='black', label='Threshold = 0.5')

ax2.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05],
        title='ROC Curve (Test Set - Standard)',
        xlabel='False Positive Rate', ylabel='True Positive Rate')
ax2.legend(loc="lower right")
plt.show()

# ==========================================
# 6. åŒ¯å‡ºæ©Ÿç‡ CSV èˆ‡ é–¾å€¼èª¿æ•´æ¸¬è©¦ (Threshold = 0.65)
# ==========================================
print("\n" + "-" * 60)
print("[5] åŒ¯å‡º CSV èˆ‡ é–¾å€¼èª¿æ•´åˆ†æ (Threshold = 0.65)")
print("-" * 60)

# 1. è£½ä½œ DataFrame ä¸¦å­˜æª”
df_result = pd.DataFrame({
    'True_Label': y_test.values,
    'Predicted_Probability': y_prob_test,
    'Pred_0.5': y_pred_std
})
csv_name = "diabetes_test_probabilities.csv"
df_result.to_csv(csv_name, index=False)

# â˜… é€™è£¡æœƒå‘Šè¨´ä½ æª”æ¡ˆåœ¨å“ªè£¡
print(f"âœ… CSV æª”æ¡ˆå·²æˆåŠŸå»ºç«‹ï¼")
print(f"ğŸ“‚ æª”æ¡ˆåç¨±: {csv_name}")
print(f"ğŸ“ å®Œæ•´è·¯å¾‘: {os.path.abspath(csv_name)}")

# 2. æ‡‰ç”¨æ–°çš„é–¾å€¼ (0.65)
NEW_THRESHOLD = 0.65
# é‚è¼¯ï¼šåªæœ‰æ©Ÿç‡ >= 0.65 æ‰æ˜¯ 1ï¼Œå¦å‰‡ç‚º 0
y_pred_new = (y_prob_test >= NEW_THRESHOLD).astype(int)

# 3. è¨ˆç®—æ–°æŒ‡æ¨™
acc_new = accuracy_score(y_test, y_pred_new)
f1_new = f1_score(y_test, y_pred_new)
# æ³¨æ„ï¼šAUC ä¸æœƒå› ç‚ºé–¾å€¼æ”¹è®Šè€Œæ”¹è®Šï¼Œå› ç‚º AUC æ˜¯çœ‹æ•´é«”æ’åº
# ä½†ç‚ºäº†å®Œæ•´æ€§æˆ‘å€‘é‚„æ˜¯å°å‡ºä¾† (æ•¸å€¼æœƒè·Ÿä¸Šé¢ä¸€æ¨£)
auc_new = roc_auc_score(y_test, y_prob_test) 

print(f"\n[èª¿æ•´å¾Œæ¸¬è©¦é›†çµæœ (Threshold = {NEW_THRESHOLD})]")
print(f"Accuracy: {acc_new:.4f}")
print(f"AUC:      {auc_new:.4f} (AUCèˆ‡é–¾å€¼ç„¡é—œï¼Œæ•¸å€¼ä¸è®Š)")
print(f"F1 Score: {f1_new:.4f}")

# 4. æ–°æ··æ·†çŸ©é™£
cm_new = confusion_matrix(y_test, y_pred_new)
tn_n, fp_n, fn_n, tp_n = cm_new.ravel()
print(f"æ–°æ··æ·†çŸ©é™£:\n TN={tn_n} | FP={fp_n} (é æœŸè®Šå°‘)\n FN={fn_n} (é æœŸè®Šå¤š)| TP={tp_n}")

# 5. ç¹ªè£½æ–° ROC æ›²ç·š (æ¨™ç¤ºå‡º 0.65 çš„é»)
fig3, ax3 = plt.subplots(figsize=(10, 8))
ax3.plot(fpr, tpr, color='green', lw=2, label=f'Test ROC (AUC = {auc_new:.2f})')
ax3.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

# æ‰¾å‡º ROC æ›²ç·šä¸Šå°æ‡‰ 0.65 çš„é»
idx_new = np.argmin(np.abs(thresholds - NEW_THRESHOLD))
ax3.scatter(fpr[idx_new], tpr[idx_new], s=100, c='red', label=f'Threshold = {NEW_THRESHOLD}')

# æŠŠåŸæœ¬ 0.5 çš„é»ä¹Ÿç•«ä¸Šå»åšæ¯”è¼ƒ
ax3.scatter(fpr[idx_05], tpr[idx_05], s=50, c='black', alpha=0.5, label='Threshold = 0.5 (Ref)')

ax3.set(xlim=[0.0, 1.0], ylim=[0.0, 1.05],
        title=f'ROC Curve (Test Set - Threshold {NEW_THRESHOLD})',
        xlabel='False Positive Rate', ylabel='True Positive Rate')
ax3.legend(loc="lower right")
plt.show()

print("=" * 60)
